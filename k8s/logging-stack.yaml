# ELK Stack (Elasticsearch, Logstash, Kibana) for Centralized Logging
# This configuration deploys a complete logging stack for FieldSync

# Elasticsearch for log storage and indexing
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: fieldsync
  labels:
    app: fieldsync
    component: elasticsearch
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: fieldsync
      component: elasticsearch
  template:
    metadata:
      labels:
        app: fieldsync
        component: elasticsearch
    spec:
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
      initContainers:
      - name: increase-vm-max-map
        image: busybox
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.9.0
        ports:
        - containerPort: 9200
          name: rest
        - containerPort: 9300
          name: inter-node
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
        env:
        - name: cluster.name
          value: fieldsync-logs
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms512m -Xmx512m"
        - name: xpack.security.enabled
          value: "false"
        - name: xpack.security.enrollment.enabled
          value: "false"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        readinessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 60
          periodSeconds: 30
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 50Gi
      storageClassName: fast-ssd

---
# Elasticsearch Service
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: fieldsync
  labels:
    app: fieldsync
    component: elasticsearch
spec:
  type: ClusterIP
  selector:
    app: fieldsync
    component: elasticsearch
  ports:
  - port: 9200
    targetPort: 9200
    name: rest
  - port: 9300
    targetPort: 9300
    name: inter-node

---
# Elasticsearch Headless Service
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-headless
  namespace: fieldsync
  labels:
    app: fieldsync
    component: elasticsearch
spec:
  clusterIP: None
  selector:
    app: fieldsync
    component: elasticsearch
  ports:
  - port: 9200
    targetPort: 9200
    name: rest
  - port: 9300
    targetPort: 9300
    name: inter-node

---
# Logstash Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-config
  namespace: fieldsync
  labels:
    app: fieldsync
    component: logstash
data:
  logstash.yml: |
    http.host: "0.0.0.0"
    path.config: /usr/share/logstash/pipeline
    xpack.monitoring.enabled: false
  
  logstash.conf: |
    input {
      beats {
        port => 5044
      }
      tcp {
        port => 5000
        codec => json_lines
      }
      http {
        port => 8080
      }
    }
    
    filter {
      # Parse application logs
      if [fields][service] == "fieldsync" {
        json {
          source => "message"
        }
        
        # Add timestamp parsing
        date {
          match => [ "timestamp", "ISO8601" ]
        }
        
        # Extract user information
        if [userId] {
          mutate {
            add_field => { "user_id" => "%{userId}" }
          }
        }
        
        # Classify log levels
        if [level] == "error" {
          mutate {
            add_tag => [ "error" ]
          }
        } else if [level] == "warn" {
          mutate {
            add_tag => [ "warning" ]
          }
        }
        
        # Extract request information
        if [method] and [url] {
          mutate {
            add_field => { "request_info" => "%{method} %{url}" }
          }
        }
        
        # Geolocate IP addresses
        if [ip] {
          geoip {
            source => "ip"
            target => "geoip"
          }
        }
      }
      
      # Parse Nginx access logs
      if [fields][service] == "nginx" {
        grok {
          match => { 
            "message" => "%{COMBINEDAPACHELOG}" 
          }
        }
        
        date {
          match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
        }
        
        mutate {
          convert => { "response" => "integer" }
          convert => { "bytes" => "integer" }
        }
      }
      
      # Parse MongoDB logs
      if [fields][service] == "mongodb" {
        grok {
          match => { 
            "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:severity} %{GREEDYDATA:log_message}" 
          }
        }
      }
      
      # Add environment and application tags
      mutate {
        add_field => {
          "environment" => "production"
          "application" => "fieldsync"
        }
      }
    }
    
    output {
      elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "fieldsync-logs-%{+YYYY.MM.dd}"
        template_name => "fieldsync"
        template => "/usr/share/logstash/templates/fieldsync-template.json"
        template_overwrite => true
      }
      
      # Output to stdout for debugging
      stdout {
        codec => rubydebug
      }
    }
  
  fieldsync-template.json: |
    {
      "index_patterns": ["fieldsync-logs-*"],
      "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 1,
        "index.refresh_interval": "30s"
      },
      "mappings": {
        "properties": {
          "@timestamp": {
            "type": "date"
          },
          "level": {
            "type": "keyword"
          },
          "message": {
            "type": "text",
            "analyzer": "standard"
          },
          "service": {
            "type": "keyword"
          },
          "userId": {
            "type": "keyword"
          },
          "traceId": {
            "type": "keyword"
          },
          "requestId": {
            "type": "keyword"
          },
          "method": {
            "type": "keyword"
          },
          "url": {
            "type": "keyword"
          },
          "statusCode": {
            "type": "integer"
          },
          "responseTime": {
            "type": "float"
          },
          "ip": {
            "type": "ip"
          },
          "userAgent": {
            "type": "text"
          },
          "geoip": {
            "properties": {
              "location": {
                "type": "geo_point"
              },
              "country_name": {
                "type": "keyword"
              },
              "city_name": {
                "type": "keyword"
              }
            }
          }
        }
      }
    }

---
# Logstash Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logstash
  namespace: fieldsync
  labels:
    app: fieldsync
    component: logstash
spec:
  replicas: 2
  selector:
    matchLabels:
      app: fieldsync
      component: logstash
  template:
    metadata:
      labels:
        app: fieldsync
        component: logstash
    spec:
      containers:
      - name: logstash
        image: docker.elastic.co/logstash/logstash:8.9.0
        ports:
        - containerPort: 5044
          name: beats
        - containerPort: 5000
          name: tcp
        - containerPort: 8080
          name: http
        volumeMounts:
        - name: logstash-config
          mountPath: /usr/share/logstash/config/logstash.yml
          subPath: logstash.yml
        - name: logstash-config
          mountPath: /usr/share/logstash/pipeline/logstash.conf
          subPath: logstash.conf
        - name: logstash-config
          mountPath: /usr/share/logstash/templates/fieldsync-template.json
          subPath: fieldsync-template.json
        env:
        - name: LS_JAVA_OPTS
          value: "-Xmx512m -Xms512m"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        readinessProbe:
          httpGet:
            path: /
            port: 9600
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /
            port: 9600
          initialDelaySeconds: 60
          periodSeconds: 30
      volumes:
      - name: logstash-config
        configMap:
          name: logstash-config

---
# Logstash Service
apiVersion: v1
kind: Service
metadata:
  name: logstash
  namespace: fieldsync
  labels:
    app: fieldsync
    component: logstash
spec:
  type: ClusterIP
  selector:
    app: fieldsync
    component: logstash
  ports:
  - port: 5044
    targetPort: 5044
    name: beats
  - port: 5000
    targetPort: 5000
    name: tcp
  - port: 8080
    targetPort: 8080
    name: http

---
# Kibana Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: fieldsync
  labels:
    app: fieldsync
    component: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fieldsync
      component: kibana
  template:
    metadata:
      labels:
        app: fieldsync
        component: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:8.9.0
        ports:
        - containerPort: 5601
          name: kibana
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch:9200"
        - name: SERVER_NAME
          value: "kibana"
        - name: SERVER_HOST
          value: "0.0.0.0"
        - name: XPACK_SECURITY_ENABLED
          value: "false"
        - name: XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY
          value: "something_at_least_32_characters_long"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        readinessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 60
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 120
          periodSeconds: 30

---
# Kibana Service
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: fieldsync
  labels:
    app: fieldsync
    component: kibana
spec:
  type: ClusterIP
  selector:
    app: fieldsync
    component: kibana
  ports:
  - port: 5601
    targetPort: 5601
    name: kibana

---
# Filebeat DaemonSet for log collection
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: fieldsync
  labels:
    app: fieldsync
    component: filebeat
spec:
  selector:
    matchLabels:
      app: fieldsync
      component: filebeat
  template:
    metadata:
      labels:
        app: fieldsync
        component: filebeat
    spec:
      serviceAccountName: filebeat
      terminationGracePeriodSeconds: 30
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: filebeat
        image: docker.elastic.co/beats/filebeat:8.9.0
        args: [
          "-c", "/etc/filebeat.yml",
          "-e",
        ]
        env:
        - name: ELASTICSEARCH_HOST
          value: elasticsearch
        - name: ELASTICSEARCH_PORT
          value: "9200"
        - name: LOGSTASH_HOST
          value: logstash
        - name: LOGSTASH_PORT
          value: "5044"
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          runAsUser: 0
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
        volumeMounts:
        - name: config
          mountPath: /etc/filebeat.yml
          readOnly: true
          subPath: filebeat.yml
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: varlog
          mountPath: /var/log
          readOnly: true
      volumes:
      - name: config
        configMap:
          defaultMode: 0640
          name: filebeat-config
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlog
        hostPath:
          path: /var/log
      - name: data
        hostPath:
          path: /var/lib/filebeat-data
          type: DirectoryOrCreate

---
# Filebeat Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: fieldsync
  labels:
    app: fieldsync
    component: filebeat
data:
  filebeat.yml: |-
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*.log
      processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
    
    # Application-specific inputs
    - type: log
      paths:
        - /var/log/fieldsync/*.log
      fields:
        service: fieldsync
        environment: production
      fields_under_root: true
      multiline.pattern: '^\d{4}-\d{2}-\d{2}'
      multiline.negate: true
      multiline.match: after
    
    processors:
    - add_cloud_metadata:
        timeout: 3s
        providers: ["aws", "gcp", "azure"]
    
    output.logstash:
      hosts: ["${LOGSTASH_HOST}:${LOGSTASH_PORT}"]
    
    logging.level: info
    logging.to_files: true
    logging.files:
      path: /var/log/filebeat
      name: filebeat
      keepfiles: 7
      permissions: 0644

---
# Filebeat ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  namespace: fieldsync
  labels:
    app: fieldsync
    component: filebeat

---
# Filebeat ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: filebeat
  labels:
    app: fieldsync
    component: filebeat
rules:
- apiGroups: [""]
  resources:
  - nodes
  - namespaces
  - events
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions"]
  resources:
  - replicasets
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources:
  - statefulsets
  - deployments
  - replicasets
  verbs: ["get", "list", "watch"]

---
# Filebeat ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: filebeat
  labels:
    app: fieldsync
    component: filebeat
subjects:
- kind: ServiceAccount
  name: filebeat
  namespace: fieldsync
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io

---
# Fluent Bit for lightweight log forwarding (alternative to Filebeat)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: fieldsync
  labels:
    app: fieldsync
    component: fluent-bit
spec:
  selector:
    matchLabels:
      app: fieldsync
      component: fluent-bit
  template:
    metadata:
      labels:
        app: fieldsync
        component: fluent-bit
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "2020"
        prometheus.io/path: /api/v1/metrics/prometheus
    spec:
      serviceAccountName: fluent-bit
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:2.1.8
        ports:
        - containerPort: 2020
          name: metrics
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc/
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
      terminationGracePeriodSeconds: 10
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-config

---
# Fluent Bit Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: fieldsync
  labels:
    app: fieldsync
    component: fluent-bit
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
        HTTP_Server   On
        HTTP_Listen   0.0.0.0
        HTTP_Port     2020

    @INCLUDE input-kubernetes.conf
    @INCLUDE filter-kubernetes.conf
    @INCLUDE output-elasticsearch.conf

  input-kubernetes.conf: |
    [INPUT]
        Name              tail
        Tag               kube.*
        Path              /var/log/containers/*.log
        Parser            docker
        DB                /var/log/flb_kube.db
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On
        Refresh_Interval  10

  filter-kubernetes.conf: |
    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Keep_Log            Off
        K8S-Logging.Parser  On
        K8S-Logging.Exclude Off

  output-elasticsearch.conf: |
    [OUTPUT]
        Name            es
        Match           *
        Host            ${FLUENT_ELASTICSEARCH_HOST}
        Port            ${FLUENT_ELASTICSEARCH_PORT}
        Logstash_Format On
        Logstash_Prefix fieldsync-logs
        Retry_Limit     False
        Type            _doc
        Time_Key        @timestamp
        Time_Key_Format %Y-%m-%dT%H:%M:%S.%L%z

  parsers.conf: |
    [PARSER]
        Name   apache
        Format regex
        Regex  ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   apache2
        Format regex
        Regex  ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^ ]*) +\S*)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   nginx
        Format regex
        Regex ^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   json
        Format json
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name        docker
        Format      json
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
        Time_Keep   On

---
# Fluent Bit ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluent-bit
  namespace: fieldsync

---
# Fluent Bit ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluent-bit-read
rules:
- apiGroups: [""]
  resources:
  - namespaces
  - pods
  verbs: ["get", "list", "watch"]

---
# Fluent Bit ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluent-bit-read
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluent-bit-read
subjects:
- kind: ServiceAccount
  name: fluent-bit
  namespace: fieldsync